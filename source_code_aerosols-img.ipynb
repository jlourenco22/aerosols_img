{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bibliotecas Gerais\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# TensorFlow\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","\n","# Carregar dados\n","train_data = pd.read_csv('train.csv')\n","test_data = pd.read_csv('test.csv')\n","\n","# Remover colunas 'id'\n","train_data = train_data.drop(columns=['id'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Função para dividir o nome do arquivo e extrair informações\n","def extract_features_from_filename(filename):\n","    parts = filename.split('_')\n","\n","    def clean_coordinate(coordinate):\n","        \"\"\"Remove hyphens and convert to float if possible.\"\"\"\n","        clean_coord = coordinate.replace('-', '')\n","        try:\n","            return float(clean_coord)\n","        except ValueError:\n","            return clean_coord\n","\n","    #print(parts)\n","\n","    if len(parts) == 11:\n","        coordinates_id1 = clean_coordinate(parts[4])\n","        coordinates_id2 = clean_coordinate(parts[5])\n","        date_time_acquisition_start = parts[8].replace('T', '')\n","        date_time_acquisition_end = parts[9].replace('T', '')\n","        return pd.Series([coordinates_id1, coordinates_id2, date_time_acquisition_start, date_time_acquisition_end], \n","                         index=['coordinates_id1', 'coordinates_id2', 'date_time_acquisition_start', 'date_time_acquisition_end'])\n","\n","    if len(parts) == 10:\n","        coordinates_id1 = clean_coordinate(parts[3])\n","        coordinates_id2 = clean_coordinate(parts[4])\n","        date_time_acquisition_start = parts[7].replace('T', '')\n","        date_time_acquisition_end = parts[8].replace('T', '')\n","        return pd.Series([coordinates_id1, coordinates_id2, date_time_acquisition_start, date_time_acquisition_end], \n","                         index=['coordinates_id1', 'coordinates_id2', 'date_time_acquisition_start', 'date_time_acquisition_end'])\n","\n","    if len(parts) == 9:\n","        coordinates_id1 = clean_coordinate(parts[2])\n","        coordinates_id2 = clean_coordinate(parts[3])\n","        date_time_acquisition_start = parts[6].replace('T', '')\n","        date_time_acquisition_end = parts[7].replace('T', '')\n","        return pd.Series([coordinates_id1, coordinates_id2, date_time_acquisition_start, date_time_acquisition_end], \n","                         index=['coordinates_id1', 'coordinates_id2', 'date_time_acquisition_start', 'date_time_acquisition_end'])\n","\n","    if len(parts) == 8:\n","        coordinates_id1 = clean_coordinate(parts[1])\n","        coordinates_id2 = clean_coordinate(parts[2])\n","        date_time_acquisition_start = parts[5].replace('T', '')\n","        date_time_acquisition_end = parts[6].replace('T', '')\n","        return pd.Series([coordinates_id1, coordinates_id2, date_time_acquisition_start, date_time_acquisition_end], \n","                         index=['coordinates_id1', 'coordinates_id2', 'date_time_acquisition_start', 'date_time_acquisition_end'])\n","\n","    return pd.Series([None]*4, index=['coordinates_id1', 'coordinates_id2', 'date_time_acquisition_start', 'date_time_acquisition_end'])\n","\n","# Aplicar a função a cada nome de arquivo no dataset\n","file_features = train_data['file_name_l1'].apply(extract_features_from_filename)\n","file_features_test = test_data['file_name_l1'].apply(extract_features_from_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# Concatenar as novas features ao dataframe original\n","\n","\n","#train_data = pd.concat([train_data, file_features], axis=1)\n","#test_data = pd.concat([test_data, file_features_test], axis=1)\n","\n","# Remover colunas desnecessárias para o modelo\n","train_data = train_data.drop(columns=['file_name_l1'])\n","test_data = test_data.drop(columns=['file_name_l1'])\n","\n","# Visualizar os dados com as novas features\n","#print(train_data.head)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Estatísticas descritivas\n","print(\"Estatísticas Descritivas do Dataset COMPLETO de Treino:\")\n","print(train_data.describe())\n","\n","\n","print(\"\\n\")\n","# Verificar valores nulos\n","print(\"Verificar Missing Data:\")\n","print(train_data.isnull().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Graficos e Visualizações\n","\n","#Visualizar a distribuição normal das features\n","df_analise_dist = train_data.melt()\n","\n","#Criar um FaceGrit com um histograma para cada feature do DataSet\n","g = sns.FacetGrid(df_analise_dist, col=\"variable\", col_wrap=4, sharex=False, sharey=False, height=4)\n","g.map(sns.histplot, \"value\", kde=False, color='blue', bins=30)\n","plt.show()\n","\n","\n","#Visualizar a distribuição de outliers\n","df_analise_box_plot = train_data.melt()\n","\n","#FaceGrit com os box Plot\n","g = sns.FacetGrid(df_analise_box_plot, col=\"variable\", col_wrap=4, sharex=False, sharey=False, height=4)\n","g.map(sns.boxplot, \"value\")\n","plt.show()\n","        \n","\n","# Visualizações (dependendo do tipo de dados, ajuste as visualizações)\n","sns.pairplot(train_data)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["###################################\n","\n","# Feature engineering (if needed)\n","# Example: create a new feature 'elevation_squared'\n","\n","#train_data['elevation_squared'] = train_data['elevation'] ** 2\n","#test_data['elevation_squared'] = test_data['elevation'] ** 2\n","\n","print(train_data.head())\n","print(test_data.head())\n","###########################################\n","\n","\n","# Train e Validation Set\n","X = train_data.drop(columns=['value_550'])\n","y = train_data['value_550']\n","\n","# Split the data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=10)\n","\n","# Normalize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_val_scaled = scaler.transform(X_val)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model definition with regularization, batch normalization, and dropout\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split, KFold\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","def create_model():\n","    model = Sequential([\n","        Dense(1280, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(X_train_scaled.shape[1],)),\n","        BatchNormalization(),\n","        Dropout(0.5),\n","        Dense(640, activation='relu', kernel_regularizer=l2(0.001)),\n","        BatchNormalization(),\n","        Dropout(0.5),\n","        Dense(320, activation='relu', kernel_regularizer=l2(0.01)),\n","        BatchNormalization(),\n","        Dropout(0.2),\n","        Dense(1)  # Output layer for regression\n","    ])\n","\n","    # Compile the model with a lower learning rate\n","    optimizer = Adam(learning_rate=0.0001)\n","    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n","    \n","    return model\n","\n","\n","# Define the KFold cross-validation\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Store validation results\n","val_mae_scores = []\n","\n","for train_index, val_index in kf.split(X_scaled):\n","    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n","    y_train, y_val = y[train_index], y[val_index]\n","\n","    # Create a new model instance\n","    model = create_model()\n","\n","    # Early stopping callback\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n","\n","    # Train the model\n","    history = model.fit(X_train, y_train, epochs=300, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=1\n","                        )\n","    \n","    # Print model summary\n","    model.summary()\n","    \n","    # Evaluate the model on the validation set\n","    val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n","    val_mae_scores.append(val_mae)\n","    print(f\"Fold Validation MAE: {val_mae}\")\n","\n","# Calculate the mean and standard deviation of the validation MAE scores\n","mean_val_mae = np.mean(val_mae_scores)\n","std_val_mae = np.std(val_mae_scores)\n","\n","print(f\"\\nMean Validation MAE: {mean_val_mae}\")\n","print(f\"Standard Deviation of Validation MAE: {std_val_mae}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Optionally, you can plot the training history\n","import matplotlib.pyplot as plt\n","\n","# Plot training & validation loss values\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","\n","# Plot training & validation MAE values\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['mae'])\n","plt.plot(history.history['val_mae'])\n","plt.title('Model MAE')\n","plt.ylabel('MAE')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scikeras.wrappers import KerasRegressor\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam, SGD\n","import numpy as np\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Define the function to create your Keras model\n","def create_model(optimizer='adam', \n","                 kernel_regularizer_1=0.0001, \n","                 kernel_regularizer_2=0.001, \n","                 kernel_regularizer_3=0.01, \n","                 dropout_rate_1=0.5, \n","                 dropout_rate_2=0.5, \n","                 dropout_rate_3=0.2):\n","    model = Sequential([\n","        Dense(1280, activation='relu', kernel_regularizer=l2(kernel_regularizer_1), input_shape=(X_scaled.shape[1],)),\n","        BatchNormalization(),\n","        Dropout(dropout_rate_1),\n","        Dense(640, activation='relu', kernel_regularizer=l2(kernel_regularizer_2)),\n","        BatchNormalization(),\n","        Dropout(dropout_rate_2),\n","        Dense(320, activation='relu', kernel_regularizer=l2(kernel_regularizer_3)),\n","        BatchNormalization(),\n","        Dropout(dropout_rate_3),\n","        Dense(1)  # Output layer for regression\n","    ])\n","    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n","    return model\n","\n","# Create a KerasRegressor based on your Keras model\n","model = KerasRegressor(model=create_model, epochs=300, batch_size=32, verbose=0)\n","\n","# Define the grid search parameters with correct prefixes\n","param_grid = {\n","    'model__optimizer': ['adam', 'sgd'],\n","    'model__kernel_regularizer_1': [0.0001, 0.001, 0.01],\n","    'model__kernel_regularizer_2': [0.0001, 0.001, 0.01],\n","    'model__kernel_regularizer_3': [0.0001, 0.001, 0.01],\n","    'model__dropout_rate_1': [0.3, 0.5, 0.7],\n","    'model__dropout_rate_2': [0.3, 0.5, 0.7],\n","    'model__dropout_rate_3': [0.2, 0.4, 0.6]\n","}\n","\n","# Create GridSearchCV\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n","\n","# Fit GridSearchCV\n","grid_result = grid_search.fit(X_scaled, y)\n","\n","# Summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Make predictions on a clean test set\n","test_data_cleaned = test_data.drop(columns=['id'])\n"," \n","predictions = model.predict(test_data_cleaned)\n","\n","# Prepare submission DataFrame\n","submission_df = pd.DataFrame({\n","    'id': test_data['id'],\n","    'value_550': predictions.flatten()\n","})\n","\n","# Save to CSV\n","submission_df.to_csv('submission.csv', index=False)\n","\n","print(\"Submission file created successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8480891,"sourceId":77031,"sourceType":"competition"}],"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
