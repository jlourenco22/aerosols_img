{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bibliotecas Gerais\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split, KFold\n","import matplotlib.pyplot as plt\n","\n","# TensorFlow\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","from tensorflow.keras.regularizers import l2, l1\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Ler dados\n","train_data = pd.read_csv('train.csv')\n","test_data = pd.read_csv('test.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## CODIGO DE TESTE ##\n","\n","\n","# Função para dividir o nome do file e extrair informações\n","def extract_features_from_filename(filename):\n","    parts = filename.split('_')\n","\n","    def clean_coordinate(coordinate):\n","        \"\"\"Remove hyphens and convert to float if possible.\"\"\"\n","        clean_coord = coordinate.replace('-', '')\n","        try:\n","            return float(clean_coord)\n","        except ValueError:\n","            return clean_coord\n","\n","\n","    if len(parts) == 11:\n","        coordinates_id1 = clean_coordinate(parts[4])\n","        coordinates_id2 = clean_coordinate(parts[5])\n","        date_time_acquisition_start = parts[8].replace('T', '')\n","        date_time_acquisition_end = parts[9].replace('T', '')\n","        return pd.Series([coordinates_id1, coordinates_id2, date_time_acquisition_start, date_time_acquisition_end], \n","                         index=['coordinates_id1', 'coordinates_id2', 'date_time_acquisition_start', 'date_time_acquisition_end'])\n","\n","    if len(parts) == 10:\n","        coordinates_id1 = clean_coordinate(parts[3])\n","        coordinates_id2 = clean_coordinate(parts[4])\n","        date_time_acquisition_start = parts[7].replace('T', '')\n","        date_time_acquisition_end = parts[8].replace('T', '')\n","        return pd.Series([coordinates_id1, coordinates_id2, date_time_acquisition_start, date_time_acquisition_end], \n","                         index=['coordinates_id1', 'coordinates_id2', 'date_time_acquisition_start', 'date_time_acquisition_end'])\n","\n","    if len(parts) == 9:\n","        coordinates_id1 = clean_coordinate(parts[2])\n","        coordinates_id2 = clean_coordinate(parts[3])\n","        date_time_acquisition_start = parts[6].replace('T', '')\n","        date_time_acquisition_end = parts[7].replace('T', '')\n","        return pd.Series([coordinates_id1, coordinates_id2, date_time_acquisition_start, date_time_acquisition_end], \n","                         index=['coordinates_id1', 'coordinates_id2', 'date_time_acquisition_start', 'date_time_acquisition_end'])\n","\n","    if len(parts) == 8:\n","        coordinates_id1 = clean_coordinate(parts[1])\n","        coordinates_id2 = clean_coordinate(parts[2])\n","        date_time_acquisition_start = parts[5].replace('T', '')\n","        date_time_acquisition_end = parts[6].replace('T', '')\n","        return pd.Series([coordinates_id1, coordinates_id2, date_time_acquisition_start, date_time_acquisition_end], \n","                         index=['coordinates_id1', 'coordinates_id2', 'date_time_acquisition_start', 'date_time_acquisition_end'])\n","\n","    return pd.Series([None]*4, index=['coordinates_id1', 'coordinates_id2', 'date_time_acquisition_start', 'date_time_acquisition_end'])\n","\n","# Aplicar a função a cada nome de arquivo no dataset\n","file_features = train_data['file_name_l1'].apply(extract_features_from_filename)\n","file_features_test = test_data['file_name_l1'].apply(extract_features_from_filename)\n","\n","# Concatenar as novas features ao dataframe original\n","\n","train_data = pd.concat([train_data, file_features], axis=1)\n","test_data = pd.concat([test_data, file_features_test], axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Remover colunas 'id' e 'file_name_l1'\n","\n","train_data = train_data.drop(columns=['id', 'file_name_l1'])\n","test_data = test_data.drop(columns=['file_name_l1'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Estatísticas descritivas\n","print(\"Estatísticas Descritivas do Dataset de Treino:\")\n","desc_stats = train_data.describe()\n","print(desc_stats.to_string())\n","print(\"\\n\")\n","\n","# Verificar valores nulos\n","print(\"Verificar Missing Data:\")\n","missing_data = train_data.isnull().sum()\n","print(missing_data.to_string())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Graficos e Visualizações\n","\n","#Visualizar a distribuição normal das features\n","df_analise_dist = train_data.melt()\n","\n","#Histograma para cada feature do DataSet\n","g = sns.FacetGrid(df_analise_dist, col=\"variable\", col_wrap=4, sharex=False, sharey=False, height=4)\n","g.map(sns.histplot, \"value\", kde=False, color='blue', bins=30)\n","plt.show()\n","\n","\n","#Visualizar a distribuição de outliers\n","df_analise_box_plot = train_data.melt()\n","\n","#FaceGrit com os box Plot\n","g = sns.FacetGrid(df_analise_box_plot, col=\"variable\", col_wrap=4, sharex=False, sharey=False, height=4)\n","g.map(sns.boxplot, \"value\")\n","plt.show()\n","        \n","\n","# Visualizações (dependendo do tipo de dados, ajuste as visualizações)\n","sns.pairplot(train_data)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Traino e Validation Set´s\n","X = train_data.drop(columns=['value_550'])\n","y = train_data['value_550']\n","\n","# Dividir os dados de treino completos em treino e validação\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=10)\n","\n","\n","# Standardizar as features dos dados\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_val_scaled = scaler.transform(X_val)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Modelo mais imporatne foi descobrir o batch_size correcto, aumentando a eficencia do modelo em termos de parametros a serem lidos e divididos.\n","\n","# aplicar cross valdiation dentro do modelo tmb foi importante\n","\n","# aplicar regularizaçao l1 ao incio mais forte e l2 no final\n","\n","# aplciar dropout para evitar overfitting de acordo copm o nr de neuronios\n","\n","# nr de neuroniuos a diminiar e regularizaçao tmb para evitar perdas de informaçao importante\n","\n","# 550 epoch para permitir minimizar o erro com uma paciencia de 25 epochs.\n","\n","# perceber o val los e vaml mean para perceber se o modelo esta a aprender bem ou nao de acordo com o validation set aplicado\n","\n","# confirmar a prendizagam nos plos a cada fold do modelo"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Criação do modelo\n","\n","def create_model():\n","    model = Sequential([\n","        Dense(200, activation=LeakyReLU(negative_slope=0.01), kernel_regularizer=l1(0.0001), input_shape=(X_train.shape[1],)),\n","        BatchNormalization(),\n","        Dropout(0.3),\n","        Dense(100, activation=LeakyReLU(negative_slope=0.01), kernel_regularizer=l2(0.001)),\n","        BatchNormalization(),\n","        Dropout(0.156),\n","        Dense(50, activation='relu'),\n","        BatchNormalization(),\n","        Dropout(0.025),\n","        Dense(1) \n","    ])\n","\n","\n","    optimizer = Adam(learning_rate=0.0003)\n","    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n","\n","    return model\n","\n","# K-Fold Cross Validation\n","kf = KFold(n_splits=10, shuffle=True, random_state=10)\n","\n","\n","val_mae_scores = []\n","\n","# Loop para cada fold\n","for train_index, val_index in kf.split(X_train_scaled):\n","    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n","    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n","\n","\n","    model = create_model()\n","\n","    # Early stopping callback\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n","\n","    # Trainar o modelo\n","    history = model.fit(X_train_fold, y_train_fold, epochs=550, batch_size=512, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping], verbose=1)\n","\n","    # Plotting do training and validation loss para cada fold\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title(f'Training and Validation Loss - Fold {len(val_mae_scores) + 1}')\n","    plt.legend()\n","    plt.show()\n","\n","    # Plotting do training and validation MAE para cada fold\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(history.history['mae'], label='Training MAE')\n","    plt.plot(history.history['val_mae'], label='Validation MAE')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('MAE')\n","    plt.title(f'Training and Validation MAE - Fold {len(val_mae_scores) + 1}')\n","    plt.legend()\n","    plt.show()\n","\n","    # Avaliar o modelo no conjunto de validação\n","    val_loss, val_mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n","    val_mae_scores.append(val_mae)\n","    print(f\"Fold Validation MAE: {val_mae}\")\n","\n","# Calcular a média e desvio padrão dos scores de validação\n","mean_val_mae = np.mean(val_mae_scores)\n","std_val_mae = np.std(val_mae_scores)\n","\n","print(f\"\\nMean Validation MAE: {mean_val_mae}\")\n","print(f\"Standard Deviation of Validation MAE: {std_val_mae}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot dos scores de loss e MAE\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['mae'])\n","plt.plot(history.history['val_mae'])\n","plt.title('Model MAE')\n","plt.ylabel('MAE')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Standardizar os dados de teste para fazer previsões\n","test_data_scaled = scaler.transform(test_data.drop(columns=['id']))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Gerar prediçoes\n","predictions = model.predict(test_data_scaled)\n","\n","# Preparar submisao em csv\n","submission = pd.DataFrame({\n","    'id': test_data['id'],\n","    'value_550': predictions.flatten()\n","})\n","\n","\n","submission.to_csv('submission.csv', index=False)\n","\n","print(\"Submission file created successfully!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## CODIGO DE TESTE ##\n","\n","from scikeras.wrappers import KerasRegressor\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam, SGD\n","import numpy as np\n","\n","\n","## CODIGO DE TESTE ##\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Define the function to create your Keras model\n","def create_model(optimizer='adam', \n","                 kernel_regularizer_1=0.0001, \n","                 kernel_regularizer_2=0.001, \n","                 kernel_regularizer_3=0.01, \n","                 dropout_rate_1=0.5, \n","                 dropout_rate_2=0.5, \n","                 dropout_rate_3=0.2):\n","    model = Sequential([\n","        Dense(1280, activation='relu', kernel_regularizer=l2(kernel_regularizer_1), input_shape=(X_scaled.shape[1],)),\n","        BatchNormalization(),\n","        Dropout(dropout_rate_1),\n","        Dense(640, activation='relu', kernel_regularizer=l2(kernel_regularizer_2)),\n","        BatchNormalization(),\n","        Dropout(dropout_rate_2),\n","        Dense(320, activation='relu', kernel_regularizer=l2(kernel_regularizer_3)),\n","        BatchNormalization(),\n","        Dropout(dropout_rate_3),\n","        Dense(1)  # Output layer for regression\n","    ])\n","    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n","    return model\n","\n","# Create a KerasRegressor based on your Keras model\n","model = KerasRegressor(model=create_model, epochs=300, batch_size=32, verbose=0)\n","\n","# Define the grid search parameters with correct prefixes\n","param_grid = {\n","    'model__optimizer': ['adam', 'sgd'],\n","    'model__kernel_regularizer_1': [0.0001, 0.001, 0.01],\n","    'model__kernel_regularizer_2': [0.0001, 0.001, 0.01],\n","    'model__kernel_regularizer_3': [0.0001, 0.001, 0.01],\n","    'model__dropout_rate_1': [0.3, 0.5, 0.7],\n","    'model__dropout_rate_2': [0.3, 0.5, 0.7],\n","    'model__dropout_rate_3': [0.2, 0.4, 0.6]\n","}\n","\n","# Create GridSearchCV\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n","\n","# Fit GridSearchCV\n","grid_result = grid_search.fit(X_scaled, y)\n","\n","# Summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Criação do modelo 0.0750\n","\n","def create_model():\n","    model = Sequential([\n","        Dense(200, activation=LeakyReLU(negative_slope=0.01), kernel_regularizer=l1(0.0001), input_shape=(X_train.shape[1],)),\n","        BatchNormalization(),\n","        Dropout(0.3),\n","        Dense(100, activation=LeakyReLU(negative_slope=0.01), kernel_regularizer=l2(0.001)),\n","        BatchNormalization(),\n","        Dropout(0.156),\n","        Dense(100, activation=LeakyReLU(negative_slope=0.01), kernel_regularizer=l2(0.001)),\n","        BatchNormalization(),\n","        Dropout(0.156),\n","        Dense(100, activation=LeakyReLU(negative_slope=0.01), kernel_regularizer=l2(0.002)),\n","        BatchNormalization(),\n","        Dropout(0.125),\n","        Dense(100, activation=LeakyReLU(negative_slope=0.01), kernel_regularizer=l2(0.002)),\n","        BatchNormalization(),\n","        Dropout(0.072),\n","        Dense(50, activation='relu'),\n","        BatchNormalization(),\n","        Dropout(0.025),\n","        Dense(1) \n","    ])\n","\n","\n","    optimizer = Adam(learning_rate=0.0004)\n","    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n","\n","    return model\n","\n","# K-Fold Cross Validation\n","kf = KFold(n_splits=10, shuffle=True, random_state=10)\n","\n","\n","val_mae_scores = []\n","\n","# Loop para cada fold\n","for train_index, val_index in kf.split(X_train_scaled):\n","    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n","    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n","\n","\n","    model = create_model()\n","\n","    # Early stopping callback\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n","\n","    # Trainar o modelo\n","    history = model.fit(X_train_fold, y_train_fold, epochs=550, batch_size=512, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping], verbose=1)\n","\n","    # Plotting do training and validation loss para cada fold\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title(f'Training and Validation Loss - Fold {len(val_mae_scores) + 1}')\n","    plt.legend()\n","    plt.show()\n","\n","    # Plotting do training and validation MAE para cada fold\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(history.history['mae'], label='Training MAE')\n","    plt.plot(history.history['val_mae'], label='Validation MAE')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('MAE')\n","    plt.title(f'Training and Validation MAE - Fold {len(val_mae_scores) + 1}')\n","    plt.legend()\n","    plt.show()\n","\n","    # Avaliar o modelo no conjunto de validação\n","    val_loss, val_mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n","    val_mae_scores.append(val_mae)\n","    print(f\"Fold Validation MAE: {val_mae}\")\n","\n","# Calcular a média e desvio padrão dos scores de validação\n","mean_val_mae = np.mean(val_mae_scores)\n","std_val_mae = np.std(val_mae_scores)\n","\n","print(f\"\\nMean Validation MAE: {mean_val_mae}\")\n","print(f\"Standard Deviation of Validation MAE: {std_val_mae}\")\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8480891,"sourceId":77031,"sourceType":"competition"}],"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
